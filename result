==========================
Epoch: 010 // loss: 0.628776 // training accuracy: 0.696
테스트 정확도: 61.071%

tp:225
tn:117
fp:163
fn:55
precision : 0.5798969072164949
recall : 0.8035714285714286



==========================
Epoch: 020 // loss: 0.687725 // training accuracy: 0.732
테스트 정확도: 75.000%

tp:212
tn:208
fp:72
fn:68
precision : 0.7464788732394366
recall : 0.7571428571428571



==========================
Epoch: 030 // loss: 0.420772 // training accuracy: 0.875
테스트 정확도: 83.036%

tp:251
tn:214
fp:66
fn:29
precision : 0.7917981072555205
recall : 0.8964285714285715



==========================
Epoch: 040 // loss: 0.367148 // training accuracy: 0.839
테스트 정확도: 85.000%

tp:219
tn:257
fp:23
fn:61
precision : 0.9049586776859504
recall : 0.7821428571428571



==========================
Epoch: 050 // loss: 0.200476 // training accuracy: 0.911
테스트 정확도: 85.536%

tp:244
tn:235
fp:45
fn:36
precision : 0.8442906574394463
recall : 0.8714285714285714



==========================
Epoch: 060 // loss: 0.216517 // training accuracy: 0.857
테스트 정확도: 88.036%

tp:228
tn:265
fp:15
fn:52
precision : 0.9382716049382716
recall : 0.8142857142857143



==========================
Epoch: 070 // loss: 0.155979 // training accuracy: 0.911
테스트 정확도: 87.321%

tp:271
tn:218
fp:62
fn:9
precision : 0.8138138138138138
recall : 0.9678571428571429



==========================
Epoch: 080 // loss: 0.161649 // training accuracy: 0.946
테스트 정확도: 88.571%

tp:279
tn:217
fp:63
fn:1
precision : 0.8157894736842105
recall : 0.9964285714285714



==========================
Epoch: 090 // loss: 0.121394 // training accuracy: 0.946
테스트 정확도: 86.786%

tp:278
tn:208
fp:72
fn:2
precision : 0.7942857142857143
recall : 0.9928571428571429



==========================
Epoch: 100 // loss: 0.106326 // training accuracy: 0.946
테스트 정확도: 94.643%

tp:270
tn:260
fp:20
fn:10
precision : 0.9310344827586207
recall : 0.9642857142857143



==========================
Epoch: 110 // loss: 0.113262 // training accuracy: 0.946
테스트 정확도: 93.036%

tp:274
tn:247
fp:33
fn:6
precision : 0.8925081433224755
recall : 0.9785714285714285



==========================
Epoch: 120 // loss: 0.058810 // training accuracy: 0.964
테스트 정확도: 94.821%

tp:272
tn:259
fp:21
fn:8
precision : 0.9283276450511946
recall : 0.9714285714285714



==========================
Epoch: 130 // loss: 0.078924 // training accuracy: 1.000
테스트 정확도: 94.821%

tp:265
tn:266
fp:14
fn:15
precision : 0.9498207885304659
recall : 0.9464285714285714



==========================
Epoch: 140 // loss: 0.069303 // training accuracy: 0.982
테스트 정확도: 94.643%

tp:269
tn:261
fp:19
fn:11
precision : 0.9340277777777778
recall : 0.9607142857142857



==========================
Epoch: 150 // loss: 0.057474 // training accuracy: 0.982
테스트 정확도: 95.000%

tp:266
tn:266
fp:14
fn:14
precision : 0.95
recall : 0.95



==========================
Epoch: 160 // loss: 0.045700 // training accuracy: 1.000
테스트 정확도: 95.893%

tp:265
tn:272
fp:8
fn:15
precision : 0.9706959706959707
recall : 0.9464285714285714



==========================
Epoch: 170 // loss: 0.049536 // training accuracy: 0.982
테스트 정확도: 96.607%

tp:271
tn:270
fp:10
fn:9
precision : 0.9644128113879004
recall : 0.9678571428571429



==========================
Epoch: 180 // loss: 0.013434 // training accuracy: 0.982
테스트 정확도: 96.250%

tp:276
tn:263
fp:17
fn:4
precision : 0.9419795221843004
recall : 0.9857142857142858



==========================
Epoch: 190 // loss: 0.058041 // training accuracy: 0.964
테스트 정확도: 96.786%

tp:269
tn:273
fp:7
fn:11
precision : 0.9746376811594203
recall : 0.9607142857142857



==========================
Epoch: 200 // loss: 0.014339 // training accuracy: 1.000
테스트 정확도: 95.357%

tp:279
tn:255
fp:25
fn:1
precision : 0.9177631578947368
recall : 0.9964285714285714



==========================
Epoch: 210 // loss: 0.052286 // training accuracy: 1.000
테스트 정확도: 94.286%

tp:279
tn:249
fp:31
fn:1
precision : 0.9
recall : 0.9964285714285714



==========================
Epoch: 220 // loss: 0.033147 // training accuracy: 1.000
테스트 정확도: 96.071%

tp:279
tn:259
fp:21
fn:1
precision : 0.93
recall : 0.9964285714285714



==========================
Epoch: 230 // loss: 0.037842 // training accuracy: 0.982
테스트 정확도: 95.179%

tp:278
tn:255
fp:25
fn:2
precision : 0.9174917491749175
recall : 0.9928571428571429



==========================
Epoch: 240 // loss: 0.045639 // training accuracy: 0.982
테스트 정확도: 96.786%

tp:269
tn:273
fp:7
fn:11
precision : 0.9746376811594203
recall : 0.9607142857142857



==========================
Epoch: 250 // loss: 0.029912 // training accuracy: 0.982
테스트 정확도: 95.893%

tp:259
tn:278
fp:2
fn:21
precision : 0.9923371647509579
recall : 0.925



==========================
Epoch: 260 // loss: 0.008281 // training accuracy: 1.000
테스트 정확도: 96.964%

tp:274
tn:269
fp:11
fn:6
precision : 0.9614035087719298
recall : 0.9785714285714285



==========================
Epoch: 270 // loss: 0.005629 // training accuracy: 1.000
테스트 정확도: 95.179%

tp:278
tn:255
fp:25
fn:2
precision : 0.9174917491749175
recall : 0.9928571428571429



==========================
Epoch: 280 // loss: 0.011559 // training accuracy: 0.964
테스트 정확도: 93.571%

tp:279
tn:245
fp:35
fn:1
precision : 0.8885350318471338
recall : 0.9964285714285714



==========================
Epoch: 290 // loss: 0.109162 // training accuracy: 0.946
테스트 정확도: 95.357%

tp:255
tn:279
fp:1
fn:25
precision : 0.99609375
recall : 0.9107142857142857



==========================
Epoch: 300 // loss: 0.009510 // training accuracy: 1.000
테스트 정확도: 97.679%

tp:277
tn:270
fp:10
fn:3
precision : 0.9651567944250871
recall : 0.9892857142857143



==========================
Epoch: 310 // loss: 0.003723 // training accuracy: 1.000
테스트 정확도: 98.393%

tp:279
tn:272
fp:8
fn:1
precision : 0.9721254355400697
recall : 0.9964285714285714



==========================
Epoch: 320 // loss: 0.021990 // training accuracy: 0.982
테스트 정확도: 98.036%

tp:273
tn:276
fp:4
fn:7
precision : 0.9855595667870036
recall : 0.975



==========================
Epoch: 330 // loss: 0.005142 // training accuracy: 1.000
테스트 정확도: 97.679%

tp:277
tn:270
fp:10
fn:3
precision : 0.9651567944250871
recall : 0.9892857142857143



==========================
Epoch: 340 // loss: 0.013573 // training accuracy: 1.000
테스트 정확도: 98.393%

tp:276
tn:275
fp:5
fn:4
precision : 0.9822064056939501
recall : 0.9857142857142858



==========================
Epoch: 350 // loss: 0.024473 // training accuracy: 1.000
테스트 정확도: 98.036%

tp:277
tn:272
fp:8
fn:3
precision : 0.9719298245614035
recall : 0.9892857142857143



==========================
Epoch: 360 // loss: 0.005301 // training accuracy: 1.000
테스트 정확도: 98.214%

tp:276
tn:274
fp:6
fn:4
precision : 0.9787234042553191
recall : 0.9857142857142858



==========================
Epoch: 370 // loss: 0.003159 // training accuracy: 1.000
테스트 정확도: 97.679%

tp:277
tn:270
fp:10
fn:3
precision : 0.9651567944250871
recall : 0.9892857142857143



==========================
Epoch: 380 // loss: 0.023491 // training accuracy: 1.000
테스트 정확도: 98.214%

tp:277
tn:273
fp:7
fn:3
precision : 0.9753521126760564
recall : 0.9892857142857143



==========================
Epoch: 390 // loss: 0.015279 // training accuracy: 1.000
테스트 정확도: 98.036%

tp:273
tn:276
fp:4
fn:7
precision : 0.9855595667870036
recall : 0.975



==========================
Epoch: 400 // loss: 0.001876 // training accuracy: 1.000
테스트 정확도: 97.321%

tp:277
tn:268
fp:12
fn:3
precision : 0.9584775086505191
recall : 0.9892857142857143



==========================
Epoch: 410 // loss: 0.021612 // training accuracy: 1.000
테스트 정확도: 98.214%

tp:277
tn:273
fp:7
fn:3
precision : 0.9753521126760564
recall : 0.9892857142857143



==========================
Epoch: 420 // loss: 0.015714 // training accuracy: 1.000
테스트 정확도: 97.857%

tp:276
tn:272
fp:8
fn:4
precision : 0.971830985915493
recall : 0.9857142857142858



==========================
Epoch: 430 // loss: 0.004345 // training accuracy: 1.000
테스트 정확도: 98.393%

tp:276
tn:275
fp:5
fn:4
precision : 0.9822064056939501
recall : 0.9857142857142858



==========================
Epoch: 440 // loss: 0.005591 // training accuracy: 1.000
테스트 정확도: 98.393%

tp:274
tn:277
fp:3
fn:6
precision : 0.9891696750902527
recall : 0.9785714285714285



==========================
Epoch: 450 // loss: 0.060493 // training accuracy: 0.982
테스트 정확도: 97.143%

tp:275
tn:269
fp:11
fn:5
precision : 0.9615384615384616
recall : 0.9821428571428571



==========================
Epoch: 460 // loss: 0.034449 // training accuracy: 1.000
테스트 정확도: 97.500%

tp:271
tn:275
fp:5
fn:9
precision : 0.9818840579710145
recall : 0.9678571428571429



==========================
Epoch: 470 // loss: 0.009338 // training accuracy: 1.000
테스트 정확도: 97.857%

tp:278
tn:270
fp:10
fn:2
precision : 0.9652777777777778
recall : 0.9928571428571429



==========================
Epoch: 480 // loss: 0.013975 // training accuracy: 1.000
테스트 정확도: 98.214%

tp:276
tn:274
fp:6
fn:4
precision : 0.9787234042553191
recall : 0.9857142857142858



==========================
Epoch: 490 // loss: 0.012968 // training accuracy: 1.000
테스트 정확도: 98.214%

tp:277
tn:273
fp:7
fn:3
precision : 0.9753521126760564
recall : 0.9892857142857143



==========================
Epoch: 500 // loss: 0.004156 // training accuracy: 1.000
테스트 정확도: 98.393%

tp:276
tn:275
fp:5
fn:4
precision : 0.9822064056939501
recall : 0.9857142857142858



==========================
Epoch: 510 // loss: 0.034665 // training accuracy: 1.000
테스트 정확도: 98.214%

tp:276
tn:274
fp:6
fn:4
precision : 0.9787234042553191
recall : 0.9857142857142858



==========================
Epoch: 520 // loss: 0.001949 // training accuracy: 1.000
테스트 정확도: 98.214%

tp:276
tn:274
fp:6
fn:4
precision : 0.9787234042553191
recall : 0.9857142857142858



==========================
Epoch: 530 // loss: 0.007351 // training accuracy: 1.000
테스트 정확도: 98.393%

tp:276
tn:275
fp:5
fn:4
precision : 0.9822064056939501
recall : 0.9857142857142858



==========================
Epoch: 540 // loss: 0.001786 // training accuracy: 1.000
테스트 정확도: 98.571%

tp:278
tn:274
fp:6
fn:2
precision : 0.9788732394366197
recall : 0.9928571428571429



==========================
Epoch: 550 // loss: 0.004622 // training accuracy: 1.000
테스트 정확도: 98.929%

tp:276
tn:278
fp:2
fn:4
precision : 0.9928057553956835
recall : 0.9857142857142858



==========================
Epoch: 560 // loss: 0.004169 // training accuracy: 1.000
테스트 정확도: 98.750%

tp:276
tn:277
fp:3
fn:4
precision : 0.989247311827957
recall : 0.9857142857142858



==========================
Epoch: 570 // loss: 0.003680 // training accuracy: 1.000
테스트 정확도: 98.571%

tp:277
tn:275
fp:5
fn:3
precision : 0.9822695035460993
recall : 0.9892857142857143



==========================
Epoch: 580 // loss: 0.002871 // training accuracy: 1.000
테스트 정확도: 98.571%

tp:276
tn:276
fp:4
fn:4
precision : 0.9857142857142858
recall : 0.9857142857142858



==========================
Epoch: 590 // loss: 0.003454 // training accuracy: 1.000
테스트 정확도: 98.571%

tp:279
tn:273
fp:7
fn:1
precision : 0.9755244755244755
recall : 0.9964285714285714



==========================
Epoch: 600 // loss: 0.011248 // training accuracy: 1.000
테스트 정확도: 98.571%

tp:276
tn:276
fp:4
fn:4
precision : 0.9857142857142858
recall : 0.9857142857142858



==========================
Epoch: 610 // loss: 0.004366 // training accuracy: 1.000
테스트 정확도: 98.750%

tp:277
tn:276
fp:4
fn:3
precision : 0.9857651245551602
recall : 0.9892857142857143



==========================
Epoch: 620 // loss: 0.002349 // training accuracy: 1.000
테스트 정확도: 98.750%

tp:276
tn:277
fp:3
fn:4
precision : 0.989247311827957
recall : 0.9857142857142858



==========================
Epoch: 630 // loss: 0.001127 // training accuracy: 1.000
테스트 정확도: 98.750%

tp:276
tn:277
fp:3
fn:4
precision : 0.989247311827957
recall : 0.9857142857142858



==========================
Epoch: 640 // loss: 0.004748 // training accuracy: 1.000
테스트 정확도: 98.571%

tp:276
tn:276
fp:4
fn:4
precision : 0.9857142857142858
recall : 0.9857142857142858



==========================
Epoch: 650 // loss: 0.001294 // training accuracy: 1.000
테스트 정확도: 98.929%

tp:276
tn:278
fp:2
fn:4
precision : 0.9928057553956835
recall : 0.9857142857142858



==========================
Epoch: 660 // loss: 0.004816 // training accuracy: 1.000
테스트 정확도: 98.929%

tp:276
tn:278
fp:2
fn:4
precision : 0.9928057553956835
recall : 0.9857142857142858



==========================
Epoch: 670 // loss: 0.003702 // training accuracy: 1.000
테스트 정확도: 98.750%

tp:274
tn:279
fp:1
fn:6
precision : 0.9963636363636363
recall : 0.9785714285714285



==========================
Epoch: 680 // loss: 0.001581 // training accuracy: 1.000
테스트 정확도: 98.750%

tp:277
tn:276
fp:4
fn:3
precision : 0.9857651245551602
recall : 0.9892857142857143



==========================
Epoch: 690 // loss: 0.002251 // training accuracy: 1.000
테스트 정확도: 99.107%

tp:277
tn:278
fp:2
fn:3
precision : 0.992831541218638
recall : 0.9892857142857143



==========================
Epoch: 700 // loss: 0.006766 // training accuracy: 1.000
테스트 정확도: 97.500%

tp:279
tn:267
fp:13
fn:1
precision : 0.9554794520547946
recall : 0.9964285714285714



==========================
Epoch: 710 // loss: 0.065211 // training accuracy: 1.000
테스트 정확도: 98.750%

tp:276
tn:277
fp:3
fn:4
precision : 0.989247311827957
recall : 0.9857142857142858



==========================
Epoch: 720 // loss: 0.006174 // training accuracy: 1.000
테스트 정확도: 98.571%

tp:275
tn:277
fp:3
fn:5
precision : 0.9892086330935251
recall : 0.9821428571428571



==========================
Epoch: 730 // loss: 0.001420 // training accuracy: 1.000
테스트 정확도: 98.750%

tp:277
tn:276
fp:4
fn:3
precision : 0.9857651245551602
recall : 0.9892857142857143



==========================
Epoch: 740 // loss: 0.004689 // training accuracy: 1.000
테스트 정확도: 98.571%

tp:276
tn:276
fp:4
fn:4
precision : 0.9857142857142858
recall : 0.9857142857142858



==========================
Epoch: 750 // loss: 0.005111 // training accuracy: 1.000
테스트 정확도: 99.107%

tp:277
tn:278
fp:2
fn:3
precision : 0.992831541218638
recall : 0.9892857142857143



==========================
Epoch: 760 // loss: 0.002083 // training accuracy: 1.000
테스트 정확도: 98.750%

tp:278
tn:275
fp:5
fn:2
precision : 0.9823321554770318
recall : 0.9928571428571429



==========================
Epoch: 770 // loss: 0.007967 // training accuracy: 1.000
테스트 정확도: 98.929%

tp:277
tn:277
fp:3
fn:3
precision : 0.9892857142857143
recall : 0.9892857142857143



==========================
Epoch: 780 // loss: 0.002251 // training accuracy: 1.000
테스트 정확도: 98.929%

tp:277
tn:277
fp:3
fn:3
precision : 0.9892857142857143
recall : 0.9892857142857143



==========================
Epoch: 790 // loss: 0.000918 // training accuracy: 1.000
테스트 정확도: 99.286%

tp:278
tn:278
fp:2
fn:2
precision : 0.9928571428571429
recall : 0.9928571428571429



==========================
Epoch: 800 // loss: 0.001843 // training accuracy: 1.000
테스트 정확도: 98.929%

tp:277
tn:277
fp:3
fn:3
precision : 0.9892857142857143
recall : 0.9892857142857143



==========================
Epoch: 810 // loss: 0.001903 // training accuracy: 1.000
테스트 정확도: 99.107%

tp:276
tn:279
fp:1
fn:4
precision : 0.9963898916967509
recall : 0.9857142857142858



==========================
Epoch: 820 // loss: 0.001369 // training accuracy: 1.000
테스트 정확도: 98.929%

tp:276
tn:278
fp:2
fn:4
precision : 0.9928057553956835
recall : 0.9857142857142858



==========================
Epoch: 830 // loss: 0.001791 // training accuracy: 1.000
테스트 정확도: 98.929%

tp:279
tn:275
fp:5
fn:1
precision : 0.9823943661971831
recall : 0.9964285714285714



==========================
Epoch: 840 // loss: 0.017026 // training accuracy: 1.000
테스트 정확도: 96.786%

tp:279
tn:263
fp:17
fn:1
precision : 0.9425675675675675
recall : 0.9964285714285714



==========================
Epoch: 850 // loss: 0.011131 // training accuracy: 1.000
테스트 정확도: 97.500%

tp:279
tn:267
fp:13
fn:1
precision : 0.9554794520547946
recall : 0.9964285714285714



==========================
Epoch: 860 // loss: 0.016759 // training accuracy: 1.000
테스트 정확도: 98.214%

tp:278
tn:272
fp:8
fn:2
precision : 0.972027972027972
recall : 0.9928571428571429



==========================
Epoch: 870 // loss: 0.050650 // training accuracy: 0.982
테스트 정확도: 98.571%

tp:273
tn:279
fp:1
fn:7
precision : 0.9963503649635036
recall : 0.975



==========================
Epoch: 880 // loss: 0.001776 // training accuracy: 1.000
테스트 정확도: 98.750%

tp:278
tn:275
fp:5
fn:2
precision : 0.9823321554770318
recall : 0.9928571428571429



==========================
Epoch: 890 // loss: 0.017935 // training accuracy: 1.000
테스트 정확도: 99.107%

tp:276
tn:279
fp:1
fn:4
precision : 0.9963898916967509
recall : 0.9857142857142858



==========================
Epoch: 900 // loss: 0.012037 // training accuracy: 1.000
테스트 정확도: 99.286%

tp:278
tn:278
fp:2
fn:2
precision : 0.9928571428571429
recall : 0.9928571428571429



==========================
Epoch: 910 // loss: 0.001032 // training accuracy: 1.000
테스트 정확도: 98.750%

tp:277
tn:276
fp:4
fn:3
precision : 0.9857651245551602
recall : 0.9892857142857143



==========================
Epoch: 920 // loss: 0.006160 // training accuracy: 1.000
테스트 정확도: 98.750%

tp:277
tn:276
fp:4
fn:3
precision : 0.9857651245551602
recall : 0.9892857142857143



==========================
Epoch: 930 // loss: 0.000794 // training accuracy: 1.000
테스트 정확도: 99.107%

tp:276
tn:279
fp:1
fn:4
precision : 0.9963898916967509
recall : 0.9857142857142858



==========================
Epoch: 940 // loss: 0.008595 // training accuracy: 1.000
테스트 정확도: 98.750%

tp:277
tn:276
fp:4
fn:3
precision : 0.9857651245551602
recall : 0.9892857142857143



==========================
Epoch: 950 // loss: 0.009226 // training accuracy: 1.000
테스트 정확도: 98.750%

tp:274
tn:279
fp:1
fn:6
precision : 0.9963636363636363
recall : 0.9785714285714285



==========================
Epoch: 960 // loss: 0.005582 // training accuracy: 0.982
테스트 정확도: 97.500%

tp:280
tn:266
fp:14
fn:0
precision : 0.9523809523809523
recall : 1.0



==========================
Epoch: 970 // loss: 0.007791 // training accuracy: 1.000
테스트 정확도: 98.929%

tp:277
tn:277
fp:3
fn:3
precision : 0.9892857142857143
recall : 0.9892857142857143



==========================
Epoch: 980 // loss: 0.005621 // training accuracy: 1.000
테스트 정확도: 99.107%

tp:279
tn:276
fp:4
fn:1
precision : 0.9858657243816255
recall : 0.9964285714285714



==========================
Epoch: 990 // loss: 0.010660 // training accuracy: 1.000
테스트 정확도: 99.107%

tp:280
tn:275
fp:5
fn:0
precision : 0.9824561403508771
recall : 1.0



==========================
Epoch: 1000 // loss: 0.014083 // training accuracy: 1.000
테스트 정확도: 98.929%

tp:276
tn:278
fp:2
fn:4
precision : 0.9928057553956835
recall : 0.9857142857142858



==========================
Epoch: 1010 // loss: 0.016772 // training accuracy: 1.000
테스트 정확도: 98.750%

tp:275
tn:278
fp:2
fn:5
precision : 0.9927797833935018
recall : 0.9821428571428571



==========================
Epoch: 1020 // loss: 0.001677 // training accuracy: 1.000
테스트 정확도: 99.107%

tp:277
tn:278
fp:2
fn:3
precision : 0.992831541218638
recall : 0.9892857142857143



==========================
Epoch: 1030 // loss: 0.005201 // training accuracy: 1.000
테스트 정확도: 99.107%

tp:276
tn:279
fp:1
fn:4
precision : 0.9963898916967509
recall : 0.9857142857142858



==========================
Epoch: 1040 // loss: 0.006681 // training accuracy: 1.000
테스트 정확도: 98.929%

tp:279
tn:275
fp:5
fn:1
precision : 0.9823943661971831
recall : 0.9964285714285714



==========================
Epoch: 1050 // loss: 0.002743 // training accuracy: 1.000
테스트 정확도: 99.464%

tp:279
tn:278
fp:2
fn:1
precision : 0.9928825622775801
recall : 0.9964285714285714



==========================
Epoch: 1060 // loss: 0.001950 // training accuracy: 0.982
테스트 정확도: 98.929%

tp:278
tn:276
fp:4
fn:2
precision : 0.9858156028368794
recall : 0.9928571428571429



==========================
Epoch: 1070 // loss: 0.000954 // training accuracy: 1.000
테스트 정확도: 99.464%

tp:279
tn:278
fp:2
fn:1
precision : 0.9928825622775801
recall : 0.9964285714285714



==========================
Epoch: 1080 // loss: 0.000997 // training accuracy: 1.000
테스트 정확도: 99.107%

tp:279
tn:276
fp:4
fn:1
precision : 0.9858657243816255
recall : 0.9964285714285714



==========================
Epoch: 1090 // loss: 0.007984 // training accuracy: 1.000
테스트 정확도: 99.286%

tp:277
tn:279
fp:1
fn:3
precision : 0.9964028776978417
recall : 0.9892857142857143



==========================
Epoch: 1100 // loss: 0.003723 // training accuracy: 1.000
테스트 정확도: 99.464%

tp:279
tn:278
fp:2
fn:1
precision : 0.9928825622775801
recall : 0.9964285714285714



==========================
Epoch: 1110 // loss: 0.023870 // training accuracy: 1.000
테스트 정확도: 99.286%

tp:279
tn:277
fp:3
fn:1
precision : 0.9893617021276596
recall : 0.9964285714285714



==========================
Epoch: 1120 // loss: 0.001176 // training accuracy: 1.000
테스트 정확도: 98.929%

tp:277
tn:277
fp:3
fn:3
precision : 0.9892857142857143
recall : 0.9892857142857143



==========================
Epoch: 1130 // loss: 0.008549 // training accuracy: 1.000
테스트 정확도: 98.929%

tp:280
tn:274
fp:6
fn:0
precision : 0.9790209790209791
recall : 1.0



==========================
Epoch: 1140 // loss: 0.010067 // training accuracy: 1.000
테스트 정확도: 99.107%

tp:277
tn:278
fp:2
fn:3
precision : 0.992831541218638
recall : 0.9892857142857143



==========================
Epoch: 1150 // loss: 0.003894 // training accuracy: 1.000
테스트 정확도: 99.107%

tp:279
tn:276
fp:4
fn:1
precision : 0.9858657243816255
recall : 0.9964285714285714



==========================
Epoch: 1160 // loss: 0.001106 // training accuracy: 1.000
테스트 정확도: 99.286%

tp:279
tn:277
fp:3
fn:1
precision : 0.9893617021276596
recall : 0.9964285714285714



==========================
Epoch: 1170 // loss: 0.002453 // training accuracy: 1.000
테스트 정확도: 99.464%

tp:279
tn:278
fp:2
fn:1
precision : 0.9928825622775801
recall : 0.9964285714285714



==========================
Epoch: 1180 // loss: 0.001949 // training accuracy: 1.000
테스트 정확도: 99.107%

tp:277
tn:278
fp:2
fn:3
precision : 0.992831541218638
recall : 0.9892857142857143



==========================
Epoch: 1190 // loss: 0.001530 // training accuracy: 1.000
테스트 정확도: 99.464%

tp:279
tn:278
fp:2
fn:1
precision : 0.9928825622775801
recall : 0.9964285714285714



==========================
Epoch: 1200 // loss: 0.000397 // training accuracy: 1.000
테스트 정확도: 99.286%

tp:278
tn:278
fp:2
fn:2
precision : 0.9928571428571429
recall : 0.9928571428571429



==========================
Epoch: 1210 // loss: 0.004238 // training accuracy: 1.000
테스트 정확도: 99.464%

tp:279
tn:278
fp:2
fn:1
precision : 0.9928825622775801
recall : 0.9964285714285714



==========================
Epoch: 1220 // loss: 0.000811 // training accuracy: 1.000
테스트 정확도: 99.643%

tp:279
tn:279
fp:1
fn:1
precision : 0.9964285714285714
recall : 0.9964285714285714



==========================
Epoch: 1230 // loss: 0.002810 // training accuracy: 0.982
테스트 정확도: 99.107%

tp:277
tn:278
fp:2
fn:3
precision : 0.992831541218638
recall : 0.9892857142857143



==========================
Epoch: 1240 // loss: 0.001238 // training accuracy: 1.000
테스트 정확도: 99.643%

tp:279
tn:279
fp:1
fn:1
precision : 0.9964285714285714
recall : 0.9964285714285714



==========================
Epoch: 1250 // loss: 0.000896 // training accuracy: 1.000
테스트 정확도: 99.286%

tp:279
tn:277
fp:3
fn:1
precision : 0.9893617021276596
recall : 0.9964285714285714



==========================
Epoch: 1260 // loss: 0.000811 // training accuracy: 1.000
테스트 정확도: 99.464%

tp:279
tn:278
fp:2
fn:1
precision : 0.9928825622775801
recall : 0.9964285714285714



==========================
Epoch: 1270 // loss: 0.003402 // training accuracy: 1.000
테스트 정확도: 99.464%

tp:280
tn:277
fp:3
fn:0
precision : 0.9893992932862191
recall : 1.0



==========================
Epoch: 1280 // loss: 0.000556 // training accuracy: 1.000
테스트 정확도: 99.286%

tp:277
tn:279
fp:1
fn:3
precision : 0.9964028776978417
recall : 0.9892857142857143



==========================
Epoch: 1290 // loss: 0.000257 // training accuracy: 1.000
테스트 정확도: 99.643%

tp:280
tn:278
fp:2
fn:0
precision : 0.9929078014184397
recall : 1.0



==========================
Epoch: 1300 // loss: 0.000309 // training accuracy: 1.000
테스트 정확도: 99.643%

tp:280
tn:278
fp:2
fn:0
precision : 0.9929078014184397
recall : 1.0



==========================
Epoch: 1310 // loss: 0.000285 // training accuracy: 1.000
테스트 정확도: 99.643%

tp:280
tn:278
fp:2
fn:0
precision : 0.9929078014184397
recall : 1.0



==========================
Epoch: 1320 // loss: 0.000384 // training accuracy: 1.000
테스트 정확도: 99.464%

tp:280
tn:277
fp:3
fn:0
precision : 0.9893992932862191
recall : 1.0



==========================
Epoch: 1330 // loss: 0.000666 // training accuracy: 1.000
테스트 정확도: 99.464%

tp:278
tn:279
fp:1
fn:2
precision : 0.996415770609319
recall : 0.9928571428571429



==========================
Epoch: 1340 // loss: 0.001173 // training accuracy: 1.000
테스트 정확도: 99.464%

tp:279
tn:278
fp:2
fn:1
precision : 0.9928825622775801
recall : 0.9964285714285714



==========================
Epoch: 1350 // loss: 0.000482 // training accuracy: 1.000
테스트 정확도: 99.464%

tp:280
tn:277
fp:3
fn:0
precision : 0.9893992932862191
recall : 1.0



==========================
Epoch: 1360 // loss: 0.000221 // training accuracy: 1.000
테스트 정확도: 99.821%

tp:280
tn:279
fp:1
fn:0
precision : 0.99644128113879
recall : 1.0



==========================
Epoch: 1370 // loss: 0.000200 // training accuracy: 1.000
테스트 정확도: 99.643%

tp:280
tn:278
fp:2
fn:0
precision : 0.9929078014184397
recall : 1.0



==========================
Epoch: 1380 // loss: 0.000167 // training accuracy: 1.000
테스트 정확도: 99.643%

tp:280
tn:278
fp:2
fn:0
precision : 0.9929078014184397
recall : 1.0



==========================
Epoch: 1390 // loss: 0.000488 // training accuracy: 1.000
테스트 정확도: 99.643%

tp:280
tn:278
fp:2
fn:0
precision : 0.9929078014184397
recall : 1.0



==========================
Epoch: 1400 // loss: 0.000318 // training accuracy: 1.000
테스트 정확도: 99.464%

tp:279
tn:278
fp:2
fn:1
precision : 0.9928825622775801
recall : 0.9964285714285714



==========================
Epoch: 1410 // loss: 0.000159 // training accuracy: 1.000
테스트 정확도: 99.643%

tp:280
tn:278
fp:2
fn:0
precision : 0.9929078014184397
recall : 1.0



==========================
Epoch: 1420 // loss: 0.000607 // training accuracy: 1.000
테스트 정확도: 99.464%

tp:280
tn:277
fp:3
fn:0
precision : 0.9893992932862191
recall : 1.0



==========================
Epoch: 1430 // loss: 0.004788 // training accuracy: 1.000
테스트 정확도: 99.643%

tp:280
tn:278
fp:2
fn:0
precision : 0.9929078014184397
recall : 1.0



==========================
Epoch: 1440 // loss: 0.001648 // training accuracy: 1.000
테스트 정확도: 99.107%

tp:277
tn:278
fp:2
fn:3
precision : 0.992831541218638
recall : 0.9892857142857143



==========================
Epoch: 1450 // loss: 0.000422 // training accuracy: 1.000
테스트 정확도: 99.286%

tp:280
tn:276
fp:4
fn:0
precision : 0.9859154929577465
recall : 1.0



==========================
Epoch: 1460 // loss: 0.006628 // training accuracy: 1.000
테스트 정확도: 99.464%

tp:278
tn:279
fp:1
fn:2
precision : 0.996415770609319
recall : 0.9928571428571429



==========================
Epoch: 1470 // loss: 0.000165 // training accuracy: 1.000
테스트 정확도: 99.821%

tp:280
tn:279
fp:1
fn:0
precision : 0.99644128113879
recall : 1.0



==========================
Epoch: 1480 // loss: 0.000336 // training accuracy: 1.000
테스트 정확도: 99.286%

tp:277
tn:279
fp:1
fn:3
precision : 0.9964028776978417
recall : 0.9892857142857143



==========================
Epoch: 1490 // loss: 0.000157 // training accuracy: 1.000
테스트 정확도: 99.643%

tp:280
tn:278
fp:2
fn:0
precision : 0.9929078014184397
recall : 1.0



==========================
Epoch: 1500 // loss: 0.000130 // training accuracy: 1.000
테스트 정확도: 99.643%

tp:280
tn:278
fp:2
fn:0
precision : 0.9929078014184397
recall : 1.0



==========================
Epoch: 1510 // loss: 0.000077 // training accuracy: 1.000
테스트 정확도: 99.464%

tp:279
tn:278
fp:2
fn:1
precision : 0.9928825622775801
recall : 0.9964285714285714



==========================
Epoch: 1520 // loss: 0.000419 // training accuracy: 1.000
테스트 정확도: 99.107%

tp:280
tn:275
fp:5
fn:0
precision : 0.9824561403508771
recall : 1.0



==========================
Epoch: 1530 // loss: 0.000555 // training accuracy: 1.000
테스트 정확도: 99.821%

tp:280
tn:279
fp:1
fn:0
precision : 0.99644128113879
recall : 1.0



==========================
Epoch: 1540 // loss: 0.000607 // training accuracy: 1.000
테스트 정확도: 99.464%

tp:279
tn:278
fp:2
fn:1
precision : 0.9928825622775801
recall : 0.9964285714285714



==========================
Epoch: 1550 // loss: 0.000789 // training accuracy: 1.000
테스트 정확도: 99.464%

tp:278
tn:279
fp:1
fn:2
precision : 0.996415770609319
recall : 0.9928571428571429



==========================
Epoch: 1560 // loss: 0.000529 // training accuracy: 1.000
테스트 정확도: 99.464%

tp:279
tn:278
fp:2
fn:1
precision : 0.9928825622775801
recall : 0.9964285714285714



==========================
Epoch: 1570 // loss: 0.000513 // training accuracy: 1.000
테스트 정확도: 99.643%

tp:280
tn:278
fp:2
fn:0
precision : 0.9929078014184397
recall : 1.0



==========================
Epoch: 1580 // loss: 0.000487 // training accuracy: 1.000
테스트 정확도: 99.643%

tp:280
tn:278
fp:2
fn:0
precision : 0.9929078014184397
recall : 1.0



==========================
Epoch: 1590 // loss: 0.009009 // training accuracy: 1.000
테스트 정확도: 99.286%

tp:278
tn:278
fp:2
fn:2
precision : 0.9928571428571429
recall : 0.9928571428571429



==========================
Epoch: 1600 // loss: 0.000449 // training accuracy: 1.000
테스트 정확도: 99.643%

tp:280
tn:278
fp:2
fn:0
precision : 0.9929078014184397
recall : 1.0



==========================
Epoch: 1610 // loss: 0.000339 // training accuracy: 1.000
테스트 정확도: 99.643%

tp:280
tn:278
fp:2
fn:0
precision : 0.9929078014184397
recall : 1.0



==========================
Epoch: 1620 // loss: 0.000604 // training accuracy: 1.000
테스트 정확도: 99.643%

tp:280
tn:278
fp:2
fn:0
precision : 0.9929078014184397
recall : 1.0



==========================
Epoch: 1630 // loss: 0.000364 // training accuracy: 1.000
테스트 정확도: 99.464%

tp:279
tn:278
fp:2
fn:1
precision : 0.9928825622775801
recall : 0.9964285714285714



==========================
Epoch: 1640 // loss: 0.000247 // training accuracy: 1.000
테스트 정확도: 99.286%

tp:279
tn:277
fp:3
fn:1
precision : 0.9893617021276596
recall : 0.9964285714285714



==========================
Epoch: 1650 // loss: 0.000282 // training accuracy: 1.000
테스트 정확도: 99.643%

tp:280
tn:278
fp:2
fn:0
precision : 0.9929078014184397
recall : 1.0



==========================
Epoch: 1660 // loss: 0.000334 // training accuracy: 1.000
테스트 정확도: 99.643%

tp:280
tn:278
fp:2
fn:0
precision : 0.9929078014184397
recall : 1.0



==========================
Epoch: 1670 // loss: 0.000505 // training accuracy: 1.000
테스트 정확도: 99.107%

tp:280
tn:275
fp:5
fn:0
precision : 0.9824561403508771
recall : 1.0



==========================
Epoch: 1680 // loss: 0.005923 // training accuracy: 1.000
테스트 정확도: 99.821%

tp:280
tn:279
fp:1
fn:0
precision : 0.99644128113879
recall : 1.0



==========================
Epoch: 1690 // loss: 0.001482 // training accuracy: 1.000
테스트 정확도: 99.821%

tp:280
tn:279
fp:1
fn:0
precision : 0.99644128113879
recall : 1.0



==========================
Epoch: 1700 // loss: 0.000831 // training accuracy: 1.000
테스트 정확도: 99.821%

tp:279
tn:280
fp:0
fn:1
precision : 1.0
recall : 0.9964285714285714



==========================
Epoch: 1710 // loss: 0.000211 // training accuracy: 1.000
테스트 정확도: 99.643%

tp:280
tn:278
fp:2
fn:0
precision : 0.9929078014184397
recall : 1.0



==========================
Epoch: 1720 // loss: 0.000295 // training accuracy: 1.000
테스트 정확도: 99.643%

tp:280
tn:278
fp:2
fn:0
precision : 0.9929078014184397
recall : 1.0



==========================
Epoch: 1730 // loss: 0.000302 // training accuracy: 1.000
테스트 정확도: 99.821%

tp:280
tn:279
fp:1
fn:0
precision : 0.99644128113879
recall : 1.0



==========================
Epoch: 1740 // loss: 0.000115 // training accuracy: 1.000
테스트 정확도: 99.821%

tp:279
tn:280
fp:0
fn:1
precision : 1.0
recall : 0.9964285714285714



==========================
Epoch: 1750 // loss: 0.000100 // training accuracy: 1.000
테스트 정확도: 99.821%

tp:279
tn:280
fp:0
fn:1
precision : 1.0
recall : 0.9964285714285714



==========================
Epoch: 1760 // loss: 0.000091 // training accuracy: 1.000
테스트 정확도: 99.821%

tp:279
tn:280
fp:0
fn:1
precision : 1.0
recall : 0.9964285714285714



==========================
Epoch: 1770 // loss: 0.000117 // training accuracy: 1.000
테스트 정확도: 100.000%

tp:280
tn:280
fp:0
fn:0
precision : 1.0
recall : 1.0



==========================
Epoch: 1780 // loss: 0.000072 // training accuracy: 1.000
테스트 정확도: 100.000%

tp:280
tn:280
fp:0
fn:0
precision : 1.0
recall : 1.0



==========================
Epoch: 1790 // loss: 0.000129 // training accuracy: 1.000
테스트 정확도: 99.821%

tp:279
tn:280
fp:0
fn:1
precision : 1.0
recall : 0.9964285714285714



==========================
Epoch: 1800 // loss: 0.000094 // training accuracy: 1.000
테스트 정확도: 99.821%

tp:279
tn:280
fp:0
fn:1
precision : 1.0
recall : 0.9964285714285714



==========================
Epoch: 1810 // loss: 0.000100 // training accuracy: 1.000
테스트 정확도: 99.821%

tp:279
tn:280
fp:0
fn:1
precision : 1.0
recall : 0.9964285714285714



==========================
Epoch: 1820 // loss: 0.000085 // training accuracy: 1.000
테스트 정확도: 99.821%

tp:279
tn:280
fp:0
fn:1
precision : 1.0
recall : 0.9964285714285714



==========================
Epoch: 1830 // loss: 0.001941 // training accuracy: 1.000
테스트 정확도: 100.000%

tp:280
tn:280
fp:0
fn:0
precision : 1.0
recall : 1.0



==========================
Epoch: 1840 // loss: 0.000056 // training accuracy: 1.000
테스트 정확도: 99.643%

tp:279
tn:279
fp:1
fn:1
precision : 0.9964285714285714
recall : 0.9964285714285714



==========================
Epoch: 1850 // loss: 0.000064 // training accuracy: 1.000
테스트 정확도: 99.821%

tp:280
tn:279
fp:1
fn:0
precision : 0.99644128113879
recall : 1.0



==========================
Epoch: 1860 // loss: 0.000055 // training accuracy: 1.000
테스트 정확도: 99.821%

tp:280
tn:279
fp:1
fn:0
precision : 0.99644128113879
recall : 1.0



==========================
Epoch: 1870 // loss: 0.000062 // training accuracy: 1.000
테스트 정확도: 99.821%

tp:279
tn:280
fp:0
fn:1
precision : 1.0
recall : 0.9964285714285714



==========================
Epoch: 1880 // loss: 0.000047 // training accuracy: 1.000
테스트 정확도: 100.000%

tp:280
tn:280
fp:0
fn:0
precision : 1.0
recall : 1.0



==========================
Epoch: 1890 // loss: 0.000075 // training accuracy: 1.000
테스트 정확도: 99.821%

tp:280
tn:279
fp:1
fn:0
precision : 0.99644128113879
recall : 1.0



==========================
Epoch: 1900 // loss: 0.000067 // training accuracy: 1.000
테스트 정확도: 99.821%

tp:280
tn:279
fp:1
fn:0
precision : 0.99644128113879
recall : 1.0



==========================
Epoch: 1910 // loss: 0.000063 // training accuracy: 1.000
테스트 정확도: 99.821%

tp:280
tn:279
fp:1
fn:0
precision : 0.99644128113879
recall : 1.0



==========================
Epoch: 1920 // loss: 0.000057 // training accuracy: 1.000
테스트 정확도: 99.821%

tp:280
tn:279
fp:1
fn:0
precision : 0.99644128113879
recall : 1.0



==========================
Epoch: 1930 // loss: 0.000052 // training accuracy: 1.000
테스트 정확도: 99.821%

tp:279
tn:280
fp:0
fn:1
precision : 1.0
recall : 0.9964285714285714



==========================
Epoch: 1940 // loss: 0.000049 // training accuracy: 1.000
테스트 정확도: 99.643%

tp:278
tn:280
fp:0
fn:2
precision : 1.0
recall : 0.9928571428571429



==========================
Epoch: 1950 // loss: 0.000051 // training accuracy: 1.000
테스트 정확도: 99.643%

tp:279
tn:279
fp:1
fn:1
precision : 0.9964285714285714
recall : 0.9964285714285714



==========================
Epoch: 1960 // loss: 0.000375 // training accuracy: 1.000
테스트 정확도: 99.643%

tp:279
tn:279
fp:1
fn:1
precision : 0.9964285714285714
recall : 0.9964285714285714



==========================
Epoch: 1970 // loss: 0.022673 // training accuracy: 1.000
테스트 정확도: 99.821%

tp:280
tn:279
fp:1
fn:0
precision : 0.99644128113879
recall : 1.0



==========================
Epoch: 1980 // loss: 0.000248 // training accuracy: 1.000
테스트 정확도: 99.464%

tp:277
tn:280
fp:0
fn:3
precision : 1.0
recall : 0.9892857142857143



==========================
Epoch: 1990 // loss: 0.203451 // training accuracy: 0.982
테스트 정확도: 97.143%

tp:265
tn:279
fp:1
fn:15
precision : 0.9962406015037594
recall : 0.9464285714285714



==========================
Epoch: 2000 // loss: 0.226838 // training accuracy: 0.857
테스트 정확도: 83.750%

tp:217
tn:252
fp:28
fn:63
precision : 0.8857142857142857
recall : 0.775




 == array_precision ==========================
[0.5798969072164949, 0.7464788732394366, 0.7917981072555205, 0.9049586776859504, 0.8442906574394463, 0.9382716049382716, 0.8138138138138138, 0.8157894736842105, 0.7942857142857143, 0.9310344827586207, 0.8925081433224755, 0.9283276450511946, 0.9498207885304659, 0.9340277777777778, 0.95, 0.9706959706959707, 0.9644128113879004, 0.9419795221843004, 0.9746376811594203, 0.9177631578947368, 0.9, 0.93, 0.9174917491749175, 0.9746376811594203, 0.9923371647509579, 0.9614035087719298, 0.9174917491749175, 0.8885350318471338, 0.99609375, 0.9651567944250871, 0.9721254355400697, 0.9855595667870036, 0.9651567944250871, 0.9822064056939501, 0.9719298245614035, 0.9787234042553191, 0.9651567944250871, 0.9753521126760564, 0.9855595667870036, 0.9584775086505191, 0.9753521126760564, 0.971830985915493, 0.9822064056939501, 0.9891696750902527, 0.9615384615384616, 0.9818840579710145, 0.9652777777777778, 0.9787234042553191, 0.9753521126760564, 0.9822064056939501, 0.9787234042553191, 0.9787234042553191, 0.9822064056939501, 0.9788732394366197, 0.9928057553956835, 0.989247311827957, 0.9822695035460993, 0.9857142857142858, 0.9755244755244755, 0.9857142857142858, 0.9857651245551602, 0.989247311827957, 0.989247311827957, 0.9857142857142858, 0.9928057553956835, 0.9928057553956835, 0.9963636363636363, 0.9857651245551602, 0.992831541218638, 0.9554794520547946, 0.989247311827957, 0.9892086330935251, 0.9857651245551602, 0.9857142857142858, 0.992831541218638, 0.9823321554770318, 0.9892857142857143, 0.9892857142857143, 0.9928571428571429, 0.9892857142857143, 0.9963898916967509, 0.9928057553956835, 0.9823943661971831, 0.9425675675675675, 0.9554794520547946, 0.972027972027972, 0.9963503649635036, 0.9823321554770318, 0.9963898916967509, 0.9928571428571429, 0.9857651245551602, 0.9857651245551602, 0.9963898916967509, 0.9857651245551602, 0.9963636363636363, 0.9523809523809523, 0.9892857142857143, 0.9858657243816255, 0.9824561403508771, 0.9928057553956835, 0.9927797833935018, 0.992831541218638, 0.9963898916967509, 0.9823943661971831, 0.9928825622775801, 0.9858156028368794, 0.9928825622775801, 0.9858657243816255, 0.9964028776978417, 0.9928825622775801, 0.9893617021276596, 0.9892857142857143, 0.9790209790209791, 0.992831541218638, 0.9858657243816255, 0.9893617021276596, 0.9928825622775801, 0.992831541218638, 0.9928825622775801, 0.9928571428571429, 0.9928825622775801, 0.9964285714285714, 0.992831541218638, 0.9964285714285714, 0.9893617021276596, 0.9928825622775801, 0.9893992932862191, 0.9964028776978417, 0.9929078014184397, 0.9929078014184397, 0.9929078014184397, 0.9893992932862191, 0.996415770609319, 0.9928825622775801, 0.9893992932862191, 0.99644128113879, 0.9929078014184397, 0.9929078014184397, 0.9929078014184397, 0.9928825622775801, 0.9929078014184397, 0.9893992932862191, 0.9929078014184397, 0.992831541218638, 0.9859154929577465, 0.996415770609319, 0.99644128113879, 0.9964028776978417, 0.9929078014184397, 0.9929078014184397, 0.9928825622775801, 0.9824561403508771, 0.99644128113879, 0.9928825622775801, 0.996415770609319, 0.9928825622775801, 0.9929078014184397, 0.9929078014184397, 0.9928571428571429, 0.9929078014184397, 0.9929078014184397, 0.9929078014184397, 0.9928825622775801, 0.9893617021276596, 0.9929078014184397, 0.9929078014184397, 0.9824561403508771, 0.99644128113879, 0.99644128113879, 1.0, 0.9929078014184397, 0.9929078014184397, 0.99644128113879, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9964285714285714, 0.99644128113879, 0.99644128113879, 1.0, 1.0, 0.99644128113879, 0.99644128113879, 0.99644128113879, 0.99644128113879, 1.0, 1.0, 0.9964285714285714, 0.9964285714285714, 0.99644128113879, 1.0, 0.9962406015037594, 0.8857142857142857]

 == array_precision ==========================
[0.8035714285714286, 0.7571428571428571, 0.8964285714285715, 0.7821428571428571, 0.8714285714285714, 0.8142857142857143, 0.9678571428571429, 0.9964285714285714, 0.9928571428571429, 0.9642857142857143, 0.9785714285714285, 0.9714285714285714, 0.9464285714285714, 0.9607142857142857, 0.95, 0.9464285714285714, 0.9678571428571429, 0.9857142857142858, 0.9607142857142857, 0.9964285714285714, 0.9964285714285714, 0.9964285714285714, 0.9928571428571429, 0.9607142857142857, 0.925, 0.9785714285714285, 0.9928571428571429, 0.9964285714285714, 0.9107142857142857, 0.9892857142857143, 0.9964285714285714, 0.975, 0.9892857142857143, 0.9857142857142858, 0.9892857142857143, 0.9857142857142858, 0.9892857142857143, 0.9892857142857143, 0.975, 0.9892857142857143, 0.9892857142857143, 0.9857142857142858, 0.9857142857142858, 0.9785714285714285, 0.9821428571428571, 0.9678571428571429, 0.9928571428571429, 0.9857142857142858, 0.9892857142857143, 0.9857142857142858, 0.9857142857142858, 0.9857142857142858, 0.9857142857142858, 0.9928571428571429, 0.9857142857142858, 0.9857142857142858, 0.9892857142857143, 0.9857142857142858, 0.9964285714285714, 0.9857142857142858, 0.9892857142857143, 0.9857142857142858, 0.9857142857142858, 0.9857142857142858, 0.9857142857142858, 0.9857142857142858, 0.9785714285714285, 0.9892857142857143, 0.9892857142857143, 0.9964285714285714, 0.9857142857142858, 0.9821428571428571, 0.9892857142857143, 0.9857142857142858, 0.9892857142857143, 0.9928571428571429, 0.9892857142857143, 0.9892857142857143, 0.9928571428571429, 0.9892857142857143, 0.9857142857142858, 0.9857142857142858, 0.9964285714285714, 0.9964285714285714, 0.9964285714285714, 0.9928571428571429, 0.975, 0.9928571428571429, 0.9857142857142858, 0.9928571428571429, 0.9892857142857143, 0.9892857142857143, 0.9857142857142858, 0.9892857142857143, 0.9785714285714285, 1.0, 0.9892857142857143, 0.9964285714285714, 1.0, 0.9857142857142858, 0.9821428571428571, 0.9892857142857143, 0.9857142857142858, 0.9964285714285714, 0.9964285714285714, 0.9928571428571429, 0.9964285714285714, 0.9964285714285714, 0.9892857142857143, 0.9964285714285714, 0.9964285714285714, 0.9892857142857143, 1.0, 0.9892857142857143, 0.9964285714285714, 0.9964285714285714, 0.9964285714285714, 0.9892857142857143, 0.9964285714285714, 0.9928571428571429, 0.9964285714285714, 0.9964285714285714, 0.9892857142857143, 0.9964285714285714, 0.9964285714285714, 0.9964285714285714, 1.0, 0.9892857142857143, 1.0, 1.0, 1.0, 1.0, 0.9928571428571429, 0.9964285714285714, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9964285714285714, 1.0, 1.0, 1.0, 0.9892857142857143, 1.0, 0.9928571428571429, 1.0, 0.9892857142857143, 1.0, 1.0, 0.9964285714285714, 1.0, 1.0, 0.9964285714285714, 0.9928571428571429, 0.9964285714285714, 1.0, 1.0, 0.9928571428571429, 1.0, 1.0, 1.0, 0.9964285714285714, 0.9964285714285714, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9964285714285714, 1.0, 1.0, 1.0, 0.9964285714285714, 0.9964285714285714, 0.9964285714285714, 1.0, 1.0, 0.9964285714285714, 0.9964285714285714, 0.9964285714285714, 0.9964285714285714, 1.0, 0.9964285714285714, 1.0, 1.0, 0.9964285714285714, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9964285714285714, 0.9928571428571429, 0.9964285714285714, 0.9964285714285714, 1.0, 0.9892857142857143, 0.9464285714285714, 0.775]
